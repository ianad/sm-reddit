{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = pd.read_csv('../data/subreddits.csv')\n",
    "submissions = pd.read_csv('../data/submissions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits['type'] = subreddits['table_number'].map({\n",
    "    0:'Games and series',\n",
    "    1:'Tabletop',\n",
    "    2:'Nonspecific',\n",
    "    3:'Genres',\n",
    "    4:'Groups',\n",
    "    5:'Platforms',\n",
    "    6:'Companies',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (subreddits.groupby('type', as_index=False)\n",
    "      .agg({'subscribers':'mean'})\n",
    "      .sort_values('subscribers', ascending=False))\n",
    "sns.barplot(data=df, x='type',y='subscribers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits['subscribers_thousands'] = subreddits['subscribers'] / 1000.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (submissions.groupby('subreddit', as_index=False)\n",
    " .agg({\"score\":\"mean\", 'num_comments':'mean'})\n",
    " .rename(columns={'score':'mean_score', 'num_comments':'mean_num_comments'})\n",
    " .merge(subreddits, left_on='subreddit', right_on='display_name')\n",
    " #.drop(columns=['Link','https_Link','title','label','table_number','display_name'])\n",
    " .sort_values('subscribers', ascending=False))\n",
    "df['subscribers_thousands'] = df['subscribers'] / 1000.00\n",
    "\n",
    "sns.scatterplot(data=df, x='subscribers_thousands', y='mean_num_comments', hue='type')\n",
    "#sns.despine(left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.groupby('type', as_index=False)\n",
    "      .agg({'mean_score':'mean', 'mean_num_comments':'mean', 'subscribers_thousands':'mean'}))\n",
    "\n",
    "sns.barplot(data=df.sort_values('mean_num_comments', ascending=False),\n",
    "            x='type', y='mean_num_comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subreddit-specific Self-Text Corpus\n",
    "df = (submissions\n",
    "      .dropna(subset=['selftext'])\n",
    "      .groupby('subreddit', as_index=False)\n",
    "      .agg({'selftext':'sum'}))\n",
    "df['corpus_len'] = df.selftext.apply(len)\n",
    "df.sort_values('corpus_len', ascending=False, inplace=True)\n",
    "subreddit_top10_corpus = df\n",
    "subreddit_top10_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = subreddit_top10_corpus.set_index('subreddit').head(20).plot.bar()\n",
    "ax.set_ylabel('corpus length')\n",
    "ax.set_title('Corpus Length of Subreddits\\' Top 10 submissions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "subreddit_top10_corpus['tokens'] = subreddit_top10_corpus['selftext'].apply(nltk.word_tokenize)\n",
    "subreddit_top10_corpus[['subreddit','tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_top10_corpus['token_ct'] = subreddit_top10_corpus['tokens'].apply(len)\n",
    "ax = (subreddit_top10_corpus.sort_values('corpus_len', ascending=False).head(20).set_index('subreddit')).plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('../data/comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['body_tokens'] = comments['body'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['body_tokens_filtered'] = comments['body_tokens'].apply(lambda tokens: [t for t in tokens if not t.lower() in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['body_tokens_lower'] = comments['body_tokens'].apply(lambda tokens: [t.lower() for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['body_tokens_lower_filtered'] = comments['body_tokens_lower'].apply(lambda tokens: [t for t in tokens if not t in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues:\n",
    "1. punctuation\n",
    "2. case\n",
    "3. links\n",
    "4. markdown syntax tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entire comment corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comments['body_tokens_filtered'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total corpus size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comments['body_tokens_lower_filtered'].sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
